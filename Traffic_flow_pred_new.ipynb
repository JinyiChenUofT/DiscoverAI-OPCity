{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic_flow_pred.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH52Een9qpo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10SyrFEHxkMK",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIjBkFpyC70q",
        "colab_type": "code",
        "outputId": "148f046d-e54e-4f95-880d-3716b9cfccac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EobU7f18D1tq",
        "colab_type": "code",
        "outputId": "80936176-68f2-46fd-9366-422f74799e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/dot_traffic_2015.txt.gz.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/dot_traffic_2015.txt.gz.zip\n",
            "  inflating: dot_traffic_2015.txt.gz  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLzwJullDCkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traffic_df = pd.read_csv('/content/dot_traffic_2015.txt.gz', compression='gzip', header=0, sep=',', quotechar='\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPRu_3rzv2r_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_df = traffic_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_xttvPXStqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del traffic_df['functional_classification_name']\n",
        "del traffic_df['date']\n",
        "del traffic_df['restrictions']\n",
        "del traffic_df['year_of_data']\n",
        "del traffic_df['record_type']\n",
        "del traffic_df['direction_of_travel_name']\n",
        "del traffic_df['fips_state_code']\n",
        "del traffic_df['station_id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHFGFZ4kEkAG",
        "colab_type": "code",
        "outputId": "a92b88f2-e086-4a83-80c5-66d6a5b515af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "traffic_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day_of_data</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>direction_of_travel</th>\n",
              "      <th>functional_classification</th>\n",
              "      <th>lane_of_travel</th>\n",
              "      <th>month_of_data</th>\n",
              "      <th>traffic_volume_counted_after_0000_to_0100</th>\n",
              "      <th>traffic_volume_counted_after_0100_to_0200</th>\n",
              "      <th>traffic_volume_counted_after_0200_to_0300</th>\n",
              "      <th>traffic_volume_counted_after_0300_to_0400</th>\n",
              "      <th>traffic_volume_counted_after_0400_to_0500</th>\n",
              "      <th>traffic_volume_counted_after_0500_to_0600</th>\n",
              "      <th>traffic_volume_counted_after_0600_to_0700</th>\n",
              "      <th>traffic_volume_counted_after_0700_to_0800</th>\n",
              "      <th>traffic_volume_counted_after_0800_to_0900</th>\n",
              "      <th>traffic_volume_counted_after_0900_to_1000</th>\n",
              "      <th>traffic_volume_counted_after_1000_to_1100</th>\n",
              "      <th>traffic_volume_counted_after_1100_to_1200</th>\n",
              "      <th>traffic_volume_counted_after_1200_to_1300</th>\n",
              "      <th>traffic_volume_counted_after_1300_to_1400</th>\n",
              "      <th>traffic_volume_counted_after_1400_to_1500</th>\n",
              "      <th>traffic_volume_counted_after_1500_to_1600</th>\n",
              "      <th>traffic_volume_counted_after_1600_to_1700</th>\n",
              "      <th>traffic_volume_counted_after_1700_to_1800</th>\n",
              "      <th>traffic_volume_counted_after_1800_to_1900</th>\n",
              "      <th>traffic_volume_counted_after_1900_to_2000</th>\n",
              "      <th>traffic_volume_counted_after_2000_to_2100</th>\n",
              "      <th>traffic_volume_counted_after_2100_to_2200</th>\n",
              "      <th>traffic_volume_counted_after_2200_to_2300</th>\n",
              "      <th>traffic_volume_counted_after_2300_to_2400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3R</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>43</td>\n",
              "      <td>78</td>\n",
              "      <td>116</td>\n",
              "      <td>144</td>\n",
              "      <td>132</td>\n",
              "      <td>115</td>\n",
              "      <td>150</td>\n",
              "      <td>184</td>\n",
              "      <td>169</td>\n",
              "      <td>136</td>\n",
              "      <td>129</td>\n",
              "      <td>89</td>\n",
              "      <td>122</td>\n",
              "      <td>124</td>\n",
              "      <td>110</td>\n",
              "      <td>69</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1U</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>381</td>\n",
              "      <td>252</td>\n",
              "      <td>218</td>\n",
              "      <td>194</td>\n",
              "      <td>220</td>\n",
              "      <td>348</td>\n",
              "      <td>453</td>\n",
              "      <td>679</td>\n",
              "      <td>826</td>\n",
              "      <td>962</td>\n",
              "      <td>1158</td>\n",
              "      <td>1379</td>\n",
              "      <td>1376</td>\n",
              "      <td>1383</td>\n",
              "      <td>1453</td>\n",
              "      <td>1617</td>\n",
              "      <td>1669</td>\n",
              "      <td>1308</td>\n",
              "      <td>1068</td>\n",
              "      <td>928</td>\n",
              "      <td>885</td>\n",
              "      <td>798</td>\n",
              "      <td>650</td>\n",
              "      <td>613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1U</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>585</td>\n",
              "      <td>408</td>\n",
              "      <td>328</td>\n",
              "      <td>364</td>\n",
              "      <td>696</td>\n",
              "      <td>1929</td>\n",
              "      <td>4228</td>\n",
              "      <td>5634</td>\n",
              "      <td>5673</td>\n",
              "      <td>4636</td>\n",
              "      <td>3925</td>\n",
              "      <td>3827</td>\n",
              "      <td>4049</td>\n",
              "      <td>3954</td>\n",
              "      <td>4077</td>\n",
              "      <td>4244</td>\n",
              "      <td>4405</td>\n",
              "      <td>4609</td>\n",
              "      <td>4361</td>\n",
              "      <td>3272</td>\n",
              "      <td>2243</td>\n",
              "      <td>2050</td>\n",
              "      <td>1453</td>\n",
              "      <td>892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1U</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>105</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "      <td>66</td>\n",
              "      <td>77</td>\n",
              "      <td>113</td>\n",
              "      <td>254</td>\n",
              "      <td>367</td>\n",
              "      <td>487</td>\n",
              "      <td>668</td>\n",
              "      <td>870</td>\n",
              "      <td>996</td>\n",
              "      <td>1003</td>\n",
              "      <td>1000</td>\n",
              "      <td>1043</td>\n",
              "      <td>1011</td>\n",
              "      <td>959</td>\n",
              "      <td>851</td>\n",
              "      <td>708</td>\n",
              "      <td>559</td>\n",
              "      <td>457</td>\n",
              "      <td>297</td>\n",
              "      <td>207</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4R</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>52</td>\n",
              "      <td>64</td>\n",
              "      <td>68</td>\n",
              "      <td>82</td>\n",
              "      <td>96</td>\n",
              "      <td>99</td>\n",
              "      <td>87</td>\n",
              "      <td>87</td>\n",
              "      <td>83</td>\n",
              "      <td>61</td>\n",
              "      <td>55</td>\n",
              "      <td>35</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>25</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2U</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1168</td>\n",
              "      <td>781</td>\n",
              "      <td>738</td>\n",
              "      <td>425</td>\n",
              "      <td>279</td>\n",
              "      <td>395</td>\n",
              "      <td>715</td>\n",
              "      <td>1202</td>\n",
              "      <td>1486</td>\n",
              "      <td>1819</td>\n",
              "      <td>2193</td>\n",
              "      <td>2401</td>\n",
              "      <td>2691</td>\n",
              "      <td>2508</td>\n",
              "      <td>2700</td>\n",
              "      <td>2673</td>\n",
              "      <td>2746</td>\n",
              "      <td>2564</td>\n",
              "      <td>2257</td>\n",
              "      <td>2140</td>\n",
              "      <td>2060</td>\n",
              "      <td>2156</td>\n",
              "      <td>1873</td>\n",
              "      <td>1590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3U</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>34</td>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>88</td>\n",
              "      <td>269</td>\n",
              "      <td>334</td>\n",
              "      <td>476</td>\n",
              "      <td>444</td>\n",
              "      <td>355</td>\n",
              "      <td>375</td>\n",
              "      <td>434</td>\n",
              "      <td>492</td>\n",
              "      <td>499</td>\n",
              "      <td>539</td>\n",
              "      <td>542</td>\n",
              "      <td>571</td>\n",
              "      <td>559</td>\n",
              "      <td>395</td>\n",
              "      <td>351</td>\n",
              "      <td>262</td>\n",
              "      <td>217</td>\n",
              "      <td>153</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4U</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>47</td>\n",
              "      <td>95</td>\n",
              "      <td>158</td>\n",
              "      <td>151</td>\n",
              "      <td>151</td>\n",
              "      <td>141</td>\n",
              "      <td>184</td>\n",
              "      <td>193</td>\n",
              "      <td>219</td>\n",
              "      <td>217</td>\n",
              "      <td>238</td>\n",
              "      <td>278</td>\n",
              "      <td>250</td>\n",
              "      <td>154</td>\n",
              "      <td>130</td>\n",
              "      <td>84</td>\n",
              "      <td>56</td>\n",
              "      <td>21</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1R</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>115</td>\n",
              "      <td>78</td>\n",
              "      <td>31</td>\n",
              "      <td>40</td>\n",
              "      <td>42</td>\n",
              "      <td>134</td>\n",
              "      <td>268</td>\n",
              "      <td>335</td>\n",
              "      <td>392</td>\n",
              "      <td>441</td>\n",
              "      <td>600</td>\n",
              "      <td>722</td>\n",
              "      <td>652</td>\n",
              "      <td>706</td>\n",
              "      <td>751</td>\n",
              "      <td>740</td>\n",
              "      <td>784</td>\n",
              "      <td>822</td>\n",
              "      <td>703</td>\n",
              "      <td>425</td>\n",
              "      <td>192</td>\n",
              "      <td>159</td>\n",
              "      <td>150</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2U</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>189</td>\n",
              "      <td>100</td>\n",
              "      <td>115</td>\n",
              "      <td>108</td>\n",
              "      <td>219</td>\n",
              "      <td>527</td>\n",
              "      <td>1212</td>\n",
              "      <td>1596</td>\n",
              "      <td>1347</td>\n",
              "      <td>1115</td>\n",
              "      <td>1114</td>\n",
              "      <td>1126</td>\n",
              "      <td>1178</td>\n",
              "      <td>1193</td>\n",
              "      <td>1364</td>\n",
              "      <td>1662</td>\n",
              "      <td>1673</td>\n",
              "      <td>1519</td>\n",
              "      <td>1148</td>\n",
              "      <td>914</td>\n",
              "      <td>808</td>\n",
              "      <td>555</td>\n",
              "      <td>468</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   day_of_data  ...  traffic_volume_counted_after_2300_to_2400\n",
              "0            7  ...                                          6\n",
              "1           26  ...                                        613\n",
              "2           16  ...                                        892\n",
              "3           26  ...                                        110\n",
              "4           23  ...                                          7\n",
              "5           25  ...                                       1590\n",
              "6           10  ...                                         61\n",
              "7           27  ...                                         14\n",
              "8           26  ...                                        168\n",
              "9           12  ...                                        270\n",
              "\n",
              "[10 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YRXqk-BZ-Zu",
        "colab_type": "code",
        "outputId": "347fc25c-d300-470c-a2f4-649a78be9873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "traffic_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7140391, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYP5L6inFKBx",
        "colab_type": "code",
        "outputId": "f9bbadd0-2c88-49be-fbf2-8844dacb15e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "one_hot_column = ['direction_of_travel', 'functional_classification','direction_of_travel','lane_of_travel','month_of_data','day_of_week','day_of_data']\n",
        "prefix = []\n",
        "for col in one_hot_column:\n",
        "  prefix.append(col.replace('_',''))\n",
        "data = pd.get_dummies(traffic_df,prefix=prefix, columns=one_hot_column)\n",
        "data = data[:30000]\n",
        "data[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>traffic_volume_counted_after_0000_to_0100</th>\n",
              "      <th>traffic_volume_counted_after_0100_to_0200</th>\n",
              "      <th>traffic_volume_counted_after_0200_to_0300</th>\n",
              "      <th>traffic_volume_counted_after_0300_to_0400</th>\n",
              "      <th>traffic_volume_counted_after_0400_to_0500</th>\n",
              "      <th>traffic_volume_counted_after_0500_to_0600</th>\n",
              "      <th>traffic_volume_counted_after_0600_to_0700</th>\n",
              "      <th>traffic_volume_counted_after_0700_to_0800</th>\n",
              "      <th>traffic_volume_counted_after_0800_to_0900</th>\n",
              "      <th>traffic_volume_counted_after_0900_to_1000</th>\n",
              "      <th>traffic_volume_counted_after_1000_to_1100</th>\n",
              "      <th>traffic_volume_counted_after_1100_to_1200</th>\n",
              "      <th>traffic_volume_counted_after_1200_to_1300</th>\n",
              "      <th>traffic_volume_counted_after_1300_to_1400</th>\n",
              "      <th>traffic_volume_counted_after_1400_to_1500</th>\n",
              "      <th>traffic_volume_counted_after_1500_to_1600</th>\n",
              "      <th>traffic_volume_counted_after_1600_to_1700</th>\n",
              "      <th>traffic_volume_counted_after_1700_to_1800</th>\n",
              "      <th>traffic_volume_counted_after_1800_to_1900</th>\n",
              "      <th>traffic_volume_counted_after_1900_to_2000</th>\n",
              "      <th>traffic_volume_counted_after_2000_to_2100</th>\n",
              "      <th>traffic_volume_counted_after_2100_to_2200</th>\n",
              "      <th>traffic_volume_counted_after_2200_to_2300</th>\n",
              "      <th>traffic_volume_counted_after_2300_to_2400</th>\n",
              "      <th>directionoftravel_0</th>\n",
              "      <th>directionoftravel_1</th>\n",
              "      <th>directionoftravel_2</th>\n",
              "      <th>directionoftravel_3</th>\n",
              "      <th>directionoftravel_4</th>\n",
              "      <th>directionoftravel_5</th>\n",
              "      <th>directionoftravel_6</th>\n",
              "      <th>directionoftravel_7</th>\n",
              "      <th>directionoftravel_8</th>\n",
              "      <th>directionoftravel_9</th>\n",
              "      <th>functionalclassification_1R</th>\n",
              "      <th>functionalclassification_1U</th>\n",
              "      <th>functionalclassification_2U</th>\n",
              "      <th>functionalclassification_3R</th>\n",
              "      <th>functionalclassification_3U</th>\n",
              "      <th>functionalclassification_4R</th>\n",
              "      <th>...</th>\n",
              "      <th>monthofdata_11</th>\n",
              "      <th>monthofdata_12</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "      <th>dayofweek_7</th>\n",
              "      <th>dayofdata_1</th>\n",
              "      <th>dayofdata_2</th>\n",
              "      <th>dayofdata_3</th>\n",
              "      <th>dayofdata_4</th>\n",
              "      <th>dayofdata_5</th>\n",
              "      <th>dayofdata_6</th>\n",
              "      <th>dayofdata_7</th>\n",
              "      <th>dayofdata_8</th>\n",
              "      <th>dayofdata_9</th>\n",
              "      <th>dayofdata_10</th>\n",
              "      <th>dayofdata_11</th>\n",
              "      <th>dayofdata_12</th>\n",
              "      <th>dayofdata_13</th>\n",
              "      <th>dayofdata_14</th>\n",
              "      <th>dayofdata_15</th>\n",
              "      <th>dayofdata_16</th>\n",
              "      <th>dayofdata_17</th>\n",
              "      <th>dayofdata_18</th>\n",
              "      <th>dayofdata_19</th>\n",
              "      <th>dayofdata_20</th>\n",
              "      <th>dayofdata_21</th>\n",
              "      <th>dayofdata_22</th>\n",
              "      <th>dayofdata_23</th>\n",
              "      <th>dayofdata_24</th>\n",
              "      <th>dayofdata_25</th>\n",
              "      <th>dayofdata_26</th>\n",
              "      <th>dayofdata_27</th>\n",
              "      <th>dayofdata_28</th>\n",
              "      <th>dayofdata_29</th>\n",
              "      <th>dayofdata_30</th>\n",
              "      <th>dayofdata_31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>43</td>\n",
              "      <td>78</td>\n",
              "      <td>116</td>\n",
              "      <td>144</td>\n",
              "      <td>132</td>\n",
              "      <td>115</td>\n",
              "      <td>150</td>\n",
              "      <td>184</td>\n",
              "      <td>169</td>\n",
              "      <td>136</td>\n",
              "      <td>129</td>\n",
              "      <td>89</td>\n",
              "      <td>122</td>\n",
              "      <td>124</td>\n",
              "      <td>110</td>\n",
              "      <td>69</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>381</td>\n",
              "      <td>252</td>\n",
              "      <td>218</td>\n",
              "      <td>194</td>\n",
              "      <td>220</td>\n",
              "      <td>348</td>\n",
              "      <td>453</td>\n",
              "      <td>679</td>\n",
              "      <td>826</td>\n",
              "      <td>962</td>\n",
              "      <td>1158</td>\n",
              "      <td>1379</td>\n",
              "      <td>1376</td>\n",
              "      <td>1383</td>\n",
              "      <td>1453</td>\n",
              "      <td>1617</td>\n",
              "      <td>1669</td>\n",
              "      <td>1308</td>\n",
              "      <td>1068</td>\n",
              "      <td>928</td>\n",
              "      <td>885</td>\n",
              "      <td>798</td>\n",
              "      <td>650</td>\n",
              "      <td>613</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>585</td>\n",
              "      <td>408</td>\n",
              "      <td>328</td>\n",
              "      <td>364</td>\n",
              "      <td>696</td>\n",
              "      <td>1929</td>\n",
              "      <td>4228</td>\n",
              "      <td>5634</td>\n",
              "      <td>5673</td>\n",
              "      <td>4636</td>\n",
              "      <td>3925</td>\n",
              "      <td>3827</td>\n",
              "      <td>4049</td>\n",
              "      <td>3954</td>\n",
              "      <td>4077</td>\n",
              "      <td>4244</td>\n",
              "      <td>4405</td>\n",
              "      <td>4609</td>\n",
              "      <td>4361</td>\n",
              "      <td>3272</td>\n",
              "      <td>2243</td>\n",
              "      <td>2050</td>\n",
              "      <td>1453</td>\n",
              "      <td>892</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>105</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "      <td>66</td>\n",
              "      <td>77</td>\n",
              "      <td>113</td>\n",
              "      <td>254</td>\n",
              "      <td>367</td>\n",
              "      <td>487</td>\n",
              "      <td>668</td>\n",
              "      <td>870</td>\n",
              "      <td>996</td>\n",
              "      <td>1003</td>\n",
              "      <td>1000</td>\n",
              "      <td>1043</td>\n",
              "      <td>1011</td>\n",
              "      <td>959</td>\n",
              "      <td>851</td>\n",
              "      <td>708</td>\n",
              "      <td>559</td>\n",
              "      <td>457</td>\n",
              "      <td>297</td>\n",
              "      <td>207</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>52</td>\n",
              "      <td>64</td>\n",
              "      <td>68</td>\n",
              "      <td>82</td>\n",
              "      <td>96</td>\n",
              "      <td>99</td>\n",
              "      <td>87</td>\n",
              "      <td>87</td>\n",
              "      <td>83</td>\n",
              "      <td>61</td>\n",
              "      <td>55</td>\n",
              "      <td>35</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 116 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   traffic_volume_counted_after_0000_to_0100  ...  dayofdata_31\n",
              "0                                          4  ...             0\n",
              "1                                        381  ...             0\n",
              "2                                        585  ...             0\n",
              "3                                        105  ...             0\n",
              "4                                          6  ...             0\n",
              "\n",
              "[5 rows x 116 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ZNeODPagsB",
        "colab_type": "code",
        "outputId": "0c740947-73f8-4d92-8622-22e371e76909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 116)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5bUGBLVU53X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_index = {} # Mapping of feature -> start index of feature in a record\n",
        "cat_values = {} # Mapping of feature -> list of categorical values the feature can take\n",
        "# build up the cat_index and cat_values dictionary\n",
        "one_hot_column = ['direction_of_travel', 'functional_classification','direction_of_travel','lane_of_travel','month_of_data','day_of_week','day_of_data']\n",
        "for i, header in enumerate(data.keys()):\n",
        "  if i > 23: # categorical header\n",
        "    feature, value = header.split('_')\n",
        "    if feature not in cat_index:\n",
        "      cat_index[feature] = i\n",
        "      cat_values[feature] = [value]\n",
        "    else:\n",
        "      cat_values[feature].append(value)\n",
        "def get_onehot(record, feature):\n",
        "  \"\"\"\n",
        "  Return the portion of `record` that is the one-hot encoding\n",
        "  of feature. For example, since the feature \"work\" is stored\n",
        "  in the indices [5:12] in each record, calling `get_range(record, \"work\")`\n",
        "  is equivalent to accessing `record[5:12]`.\n",
        "  Args:\n",
        "  - record: a numpy array representing one record, formatted\n",
        "  the same way as a row in `data.np`\n",
        "  - feature: a string, should be an element of `catcols`\n",
        "  \"\"\"\n",
        "  start_index = cat_index[feature]\n",
        "  stop_index = cat_index[feature] + len(cat_values[feature])\n",
        "  return record[start_index:stop_index]\n",
        "def get_categorical_value(onehot, feature):\n",
        "  \"\"\"\n",
        "  Return the categorical value name of a feature given\n",
        "  a one-hot vector representing the feature.\n",
        "  Args:\n",
        "  - onehot: a numpy array one-hot representation of the feature\n",
        "  - feature: a string, should be an element of `catcols`\n",
        "  Examples:\n",
        "  >>> get_categorical_value(np.array([0., 0., 0., 0., 0., 1., 0.]), \"work\")\n",
        "  'State-gov'\n",
        "  >>> get_categorical_value(np.array([0.1, 0., 1.1, 0.2, 0., 1., 0.]), \"work\")\n",
        "  'Private'\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "  idx = np.argmax(onehot)\n",
        "  return cat_values[feature][idx]\n",
        "def get_feature(record, feature):\n",
        "  \"\"\"\n",
        "  Return the categorical feature value of a record\n",
        "  \"\"\"\n",
        "  onehot = get_onehot(record, feature)\n",
        "  return get_categorical_value(onehot, feature)\n",
        "def get_features(record):\n",
        "  return { f: get_feature(record, f) for f in catcols }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J0vvE8pYK2s",
        "colab_type": "code",
        "outputId": "c4246ec1-6a1e-47e6-f773-34c00cf1367c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_categorical_value(np.array([0.1, 0., 1.1, 3.3, 0., 1., 0.,0,0,0,0,0,0,0,55,0,0,0]), \"dayofdata\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pNXZ7DVbVfx",
        "colab_type": "text"
      },
      "source": [
        "#DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrsFBjwBZB5O",
        "colab_type": "code",
        "outputId": "07e05d56-1a6c-49f5-9b4a-f437e4e387ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "np.random.seed(50) # set the numpy seed for consistent split\n",
        "arr = np.arange(len(data))\n",
        "np.random.shuffle(arr)\n",
        "np_data=data.values\n",
        "shuffled_data=np_data\n",
        "total_data=len(np_data)\n",
        "train_data=shuffled_data[:int(total_data*0.7)].astype(np.float32)\n",
        "validation_data=shuffled_data[int(total_data*0.7):int(total_data*0.85)].astype(np.float32)\n",
        "test_data=shuffled_data[int(total_data*0.85):].astype(np.float32)\n",
        "print(\"Training dataset size: {}\".format(train_data.shape[0]))\n",
        "print(\"Validation dataset size: {}\".format(validation_data.shape[0]))\n",
        "print(\"Test dataset size: {}\".format(test_data.shape[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset size: 21000\n",
            "Validation dataset size: 4500\n",
            "Test dataset size: 4500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZV9VclvxBkk",
        "colab_type": "text"
      },
      "source": [
        "#Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA5R6RKGxBCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAN0lAyGyH0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch, momentum):\n",
        "  \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "  Args:\n",
        "  config: Configuration object containing the hyperparameters\n",
        "  Returns:\n",
        "  path: A string with the hyperparameter name and value concatenated\n",
        "  \"\"\"\n",
        "  if momentum:\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}_momentum{4}\".format(name,\n",
        "            batch_size,\n",
        "            learning_rate,\n",
        "            epoch,momentum)\n",
        "  else:\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "            batch_size,\n",
        "            learning_rate,\n",
        "            epoch)\n",
        "  return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMwy_yq5xdhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.fc1 = nn.Linear(116, 100) # TODO\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=100)\n",
        "    self.act = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(100, 80)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=80)\n",
        "    self.fc3 = nn.Linear(80, 60)\n",
        "    \n",
        "    self.fc4 = nn.Linear(140, 80)\n",
        "    self.bn4 = nn.BatchNorm1d(num_features=80)\n",
        "    self.fc5 = nn.Linear(180, 100)# TODO\n",
        "    self.bn5 = nn.BatchNorm1d(num_features=100)\n",
        "    self.fc6 = nn.Linear(100, 116)# TODO\n",
        "    \n",
        "    self.sig = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    out1 = self.act(self.bn1(self.fc1(x)))\n",
        "    out2 = self.act(self.bn2(self.fc2(out1)))\n",
        "    out3 = self.act(self.fc3(out2))\n",
        "    out4 = self.act(self.bn4(self.fc4(torch.cat((out3,out2),1))))\n",
        "    out5 = self.act(self.bn5(self.fc5(torch.cat((out4,out1),1))))\n",
        "    x = self.act(self.fc6(out5))\n",
        "    x[:,24:] = self.sig(x[:,24:])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HrXn4gcqHPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    \"\"\"Return the \"accuracy\" of the autoencoder model across a data set\n",
        "    \n",
        "    Args:\n",
        "       - model: the autoencoder model, an instance of nn.Module\n",
        "       - data_loader: an instance of torch.utils.data.DataLoader\n",
        "\n",
        "    Example (to illustrate how get_accuracy is intended to be called.\n",
        "             depending on your variable naming this code might not work\n",
        "             out of the box)\n",
        "\n",
        "        >>> model = AutoEncoder()\n",
        "        >>> vdl = torch.utils.data.DataLoader(data_valid, batch_size=256, shuffle=True)\n",
        "        >>> get_accuracy(model, vdl)\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    for j in range(24):\n",
        "        for item in data_loader: # minibatches\n",
        "            inp = item.detach().numpy()\n",
        "            out = model(zero_out_feature(item.clone().to(device))).cpu().detach().numpy()\n",
        "            for i in range(out.shape[0]): # record in minibatch\n",
        "                acc += np.absolute(int(out[i][j]) - inp[i][j] )\n",
        "                count += 1\n",
        "    return (acc)/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H8yQsG38pX2n",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_training_curve(path):\n",
        "    train_err = np.loadtxt(\"{}_train_acc.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_acc.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Accuracy\")\n",
        "    plt.plot(range(1,train_err.shape[0]+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,val_err.shape[0]+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,train_loss.shape[0]+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,val_loss.shape[0]+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "def zero_out_feature(records):\n",
        "    \"\"\" Set the feature missing in records, by setting the appropriate\n",
        "    columns of records to 0\n",
        "    \"\"\"\n",
        "    col = random.randint(0, 23)\n",
        "    records[:, col:24] = 0\n",
        "    return records\n",
        "\n",
        "def zero_out_random_feature(records):\n",
        "    \"\"\" Set one random feature missing in records, by setting the \n",
        "    appropriate columns of records to 0\n",
        "    \"\"\"\n",
        "    return zero_out_feature(records)\n",
        "def validation_eval(model, valid_loader, criterion):\n",
        "    valid_error = 0\n",
        "    total_valid_loss = 0\n",
        "    for data_val in valid_loader:\n",
        "        recon=model(data_val.to(device))\n",
        "        loss = criterion(recon[:,:24], (data_val[:,:24]).to(device))\n",
        "        total_valid_loss += loss.item()\n",
        "    return total_valid_loss/len(valid_loader),get_accuracy(model, valid_loader)\n",
        "        \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4, batch_size=1,betas=(0.9, 0.999)):\n",
        "    \"\"\" Training loop. You should update this.\"\"\"\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,betas=betas)\n",
        "    train_err = []    \n",
        "    train_loss = []  \n",
        "    val_err = []  \n",
        "    val_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_error = 0\n",
        "        total_train_loss = 0\n",
        "        train_idx = 0\n",
        "        for data in train_loader:\n",
        "            data_zero = zero_out_feature(data.clone()).to(device) # zero out one categorical feature\n",
        "            recon = model(data_zero)\n",
        "            loss = criterion(recon[:,:24], (data[:,:24]).to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_train_loss += loss.item()\n",
        "            train_idx+=1\n",
        "        train_err.append(get_accuracy(model,  train_loader))\n",
        "        train_loss.append(total_train_loss/(train_idx+1))\n",
        "        valid_loss,valid_err = validation_eval(model, valid_loader,criterion)\n",
        "        val_loss.append(valid_loss)\n",
        "        val_err.append(valid_err)\n",
        "        train_loss.append(total_train_loss/len(train_loader))\n",
        "        valid_loss,_= validation_eval(model, valid_loader,criterion)\n",
        "        val_loss.append(valid_loss)\n",
        "        print((\"Epoch {}: Train loss: {} Train flow loss: {} |\"+               \n",
        "               \"Validation loss: {} Valid flow loss: {}\").format(\n",
        "            epoch + 1, train_loss[-1],train_err[-1],val_loss[-1],val_err[-1]))\n",
        "        model_path = \"Encoder_Decoder_Batchsize{}_lr{}_epoch{}_betas{}\".format(batch_size,learning_rate,epoch,betas)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    print('Finished Training')    \n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), np.array(train_loss))       \n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), np.array(val_loss))\n",
        "    return model_path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aby3mMinRRI",
        "colab_type": "code",
        "outputId": "e68011aa-72c6-44a8-c3c6-b77eb854fe59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "Encoder=AutoEncoder().to(device)\n",
        "batch_size=16\n",
        "train_loader =torch.utils.data.DataLoader(train_data,batch_size=batch_size) \n",
        "valid_loader =torch.utils.data.DataLoader(validation_data,batch_size=batch_size) \n",
        "test_loader =torch.utils.data.DataLoader(test_data,batch_size=batch_size) \n",
        "#Encoder.load_state_dict(torch.load('Encoder_Decoder_Batchsize16_lr0.01_epoch5_betas(0.9, 0.999)'))\n",
        "path = train(Encoder, train_loader, valid_loader, num_epochs=30, learning_rate=1e-2, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train loss: 59935.86985724189 Train flow loss: 230.04231547619048 |Validation loss: 38283.43357989805 Valid flow loss: 234.20361111111112\n",
            "Epoch 2: Train loss: 42854.662386985015 Train flow loss: 218.1519126984127 |Validation loss: 32404.680387023494 Valid flow loss: 220.83524074074074\n",
            "Epoch 3: Train loss: 40781.834249712905 Train flow loss: 210.01002579365078 |Validation loss: 28235.741488842254 Valid flow loss: 211.72708333333333\n",
            "Epoch 4: Train loss: 38680.80835388602 Train flow loss: 199.33608928571428 |Validation loss: 28896.306806848403 Valid flow loss: 199.78192592592592\n",
            "Epoch 5: Train loss: 36615.520524547195 Train flow loss: 181.5893888888889 |Validation loss: 27757.912007216866 Valid flow loss: 185.63072222222223\n",
            "Epoch 6: Train loss: 35744.83666611008 Train flow loss: 174.4555376984127 |Validation loss: 22221.39323479402 Valid flow loss: 175.30051851851852\n",
            "Epoch 7: Train loss: 34485.59338230154 Train flow loss: 168.30998809523808 |Validation loss: 25527.71010421861 Valid flow loss: 170.79912037037036\n",
            "Epoch 8: Train loss: 33032.902392652504 Train flow loss: 161.8557896825397 |Validation loss: 21916.181336314967 Valid flow loss: 162.21442592592592\n",
            "Epoch 9: Train loss: 32292.582192274967 Train flow loss: 167.23385317460318 |Validation loss: 21373.022461370372 Valid flow loss: 170.08626851851852\n",
            "Epoch 10: Train loss: 33136.862397007244 Train flow loss: 163.6737361111111 |Validation loss: 19994.45008363115 Valid flow loss: 165.15585185185185\n",
            "Epoch 11: Train loss: 33692.11786162935 Train flow loss: 165.52505357142857 |Validation loss: 24929.21776607865 Valid flow loss: 166.59218518518517\n",
            "Epoch 12: Train loss: 31555.072254747447 Train flow loss: 160.57298214285714 |Validation loss: 18828.142673357157 Valid flow loss: 161.37649074074073\n",
            "Epoch 13: Train loss: 32350.812038587235 Train flow loss: 160.391626984127 |Validation loss: 19044.53919192916 Valid flow loss: 162.90185185185186\n",
            "Epoch 14: Train loss: 33983.921583444404 Train flow loss: 159.61816865079365 |Validation loss: 20801.26741752895 Valid flow loss: 159.37164814814815\n",
            "Epoch 15: Train loss: 30936.360663850526 Train flow loss: 158.88847817460316 |Validation loss: 18566.970820433704 Valid flow loss: 160.15475925925927\n",
            "Epoch 16: Train loss: 31713.75741486502 Train flow loss: 158.5897003968254 |Validation loss: 16751.11690656513 Valid flow loss: 159.05628703703704\n",
            "Epoch 17: Train loss: 32129.383197398016 Train flow loss: 157.79048412698413 |Validation loss: 19547.15075986605 Valid flow loss: 159.53101851851852\n",
            "Epoch 18: Train loss: 31726.408495610303 Train flow loss: 157.19837698412698 |Validation loss: 17561.461836578153 Valid flow loss: 157.94492592592593\n",
            "Epoch 19: Train loss: 32057.35044703952 Train flow loss: 155.29420634920635 |Validation loss: 18217.60383854859 Valid flow loss: 157.69455555555555\n",
            "Epoch 20: Train loss: 32415.102060543155 Train flow loss: 155.65515476190475 |Validation loss: 17217.389906430075 Valid flow loss: 157.44101851851852\n",
            "Epoch 21: Train loss: 30280.96224736187 Train flow loss: 157.07358333333335 |Validation loss: 19875.11453268714 Valid flow loss: 160.20447222222222\n",
            "Epoch 22: Train loss: 30654.82790438092 Train flow loss: 155.02662896825396 |Validation loss: 17280.15589114791 Valid flow loss: 155.88450925925926\n",
            "Epoch 23: Train loss: 30374.904629151693 Train flow loss: 155.18213095238096 |Validation loss: 19049.358140661363 Valid flow loss: 155.89882407407407\n",
            "Epoch 24: Train loss: 31583.575873625523 Train flow loss: 156.3296130952381 |Validation loss: 18735.262291008698 Valid flow loss: 159.08622222222223\n",
            "Epoch 25: Train loss: 31646.83985248923 Train flow loss: 154.6721488095238 |Validation loss: 18255.86313840176 Valid flow loss: 157.27547222222222\n",
            "Epoch 26: Train loss: 30208.717875612117 Train flow loss: 156.71980357142857 |Validation loss: 21514.538286357907 Valid flow loss: 159.22504629629628\n",
            "Epoch 27: Train loss: 31780.708233823872 Train flow loss: 156.3721130952381 |Validation loss: 18611.588662005484 Valid flow loss: 158.00937037037036\n",
            "Epoch 28: Train loss: 30021.913621726693 Train flow loss: 153.4172142857143 |Validation loss: 16491.293212024877 Valid flow loss: 153.54755555555556\n",
            "Epoch 29: Train loss: 31909.740318420365 Train flow loss: 152.94865079365078 |Validation loss: 16246.832119339746 Valid flow loss: 153.47396296296296\n",
            "Epoch 30: Train loss: 30570.620408278246 Train flow loss: 153.94641666666666 |Validation loss: 19084.105609001 Valid flow loss: 153.8329537037037\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCUi5Dythkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/Encoder_Decoder_Batchsize16_lr0\\.01_epoch28_betas\\(0.9,\\ 0.999\\) /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUrVRxjiupx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_out_feature_fixed(records, col):\n",
        "    \"\"\" Set the feature missing in records, by setting the appropriate\n",
        "    columns of records to 0\n",
        "    \"\"\"\n",
        "    records[:, col:24] = 0\n",
        "    return records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wjF_RmmRzHNp",
        "colab": {}
      },
      "source": [
        "class AutoEncoder_2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AutoEncoder_2, self).__init__()\n",
        "    self.fc1 = nn.Linear(116, 100) # TODO\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=100)\n",
        "    self.act = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(100, 80)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=80)\n",
        "    self.fc5 = nn.Linear(180, 100)# TODO\n",
        "    self.bn5 = nn.BatchNorm1d(num_features=100)\n",
        "    self.fc6 = nn.Linear(100, 116)# TODO\n",
        "    \n",
        "    self.sig = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    out1 = self.act(self.bn1(self.fc1(x)))\n",
        "    out2 = self.act(self.bn2(self.fc2(out1)))\n",
        "    out5 = self.act(self.bn5(self.fc5(torch.cat((out2,out1),1))))\n",
        "    x = self.act(self.fc6(out5))\n",
        "    x[:,24:] = self.sig(x[:,24:])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkVmE9TBktBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_2(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4, batch_size=1,betas=(0.9, 0.999)):\n",
        "    \"\"\" Training loop. You should update this.\"\"\"\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,betas=betas)\n",
        "    train_err = []    \n",
        "    train_loss = []  \n",
        "    val_err = []  \n",
        "    val_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_error = 0\n",
        "        total_train_loss = 0\n",
        "        train_idx = 0\n",
        "        for data in train_loader:\n",
        "            data_zero = zero_out_feature(data.clone()).to(device) # zero out one categorical feature\n",
        "            recon = model(data_zero)\n",
        "            loss = criterion(recon[:,:24], (data[:,:24]).to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_train_loss += loss.item()\n",
        "            train_idx+=1\n",
        "        train_err.append(get_accuracy(model,  train_loader))\n",
        "        train_loss.append(total_train_loss/(train_idx+1))\n",
        "        valid_loss,valid_err = validation_eval(model, valid_loader,criterion)\n",
        "        val_loss.append(valid_loss)\n",
        "        val_err.append(valid_err)\n",
        "        train_loss.append(total_train_loss/len(train_loader))\n",
        "        valid_loss,_= validation_eval(model, valid_loader,criterion)\n",
        "        val_loss.append(valid_loss)\n",
        "        print((\"Epoch {}: Train loss: {} Train flow loss: {} |\"+               \n",
        "               \"Validation loss: {} Valid flow loss: {}\").format(\n",
        "            epoch + 1, train_loss[-1],train_err[-1],val_loss[-1],val_err[-1]))\n",
        "        model_path = \"Encoder_Decoder_2_Batchsize{}_lr{}_epoch{}_betas{}\".format(batch_size,learning_rate,epoch,betas)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    print('Finished Training')    \n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), np.array(train_loss))       \n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), np.array(val_loss))\n",
        "    return model_path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQX8aP370Y2h",
        "colab_type": "code",
        "outputId": "7acef069-8efe-4631-da76-058577a8aeac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "Encoder_2=AutoEncoder_2().to(device)\n",
        "batch_size=32\n",
        "train_loader =torch.utils.data.DataLoader(train_data,batch_size=batch_size) \n",
        "valid_loader =torch.utils.data.DataLoader(validation_data,batch_size=batch_size) \n",
        "test_loader =torch.utils.data.DataLoader(test_data,batch_size=batch_size) \n",
        "Encoder_2.load_state_dict(torch.load('Encoder_Decoder_2_Batchsize32_lr0.01_epoch29_betas(0.9, 0.999)'))\n",
        "path = train_2(Encoder_2, train_loader, valid_loader, num_epochs=30, learning_rate=1e-2, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train loss: 105445.09206739916 Train flow loss: 127.52561111111112 |Validation loss: 37702.07555546321 Valid flow loss: 124.41466666666666\n",
            "Epoch 2: Train loss: 106411.21996810193 Train flow loss: 126.70692658730158 |Validation loss: 39444.38995144892 Valid flow loss: 123.21080555555555\n",
            "Epoch 3: Train loss: 107895.85908515268 Train flow loss: 126.85868253968253 |Validation loss: 37627.51861702128 Valid flow loss: 123.89842592592592\n",
            "Epoch 4: Train loss: 101616.1810869423 Train flow loss: 126.18658333333333 |Validation loss: 40532.885932651814 Valid flow loss: 123.37869444444445\n",
            "Epoch 5: Train loss: 101551.034307146 Train flow loss: 124.49466468253968 |Validation loss: 37734.546488876884 Valid flow loss: 122.73361111111112\n",
            "Epoch 6: Train loss: 105231.04147715111 Train flow loss: 127.44966468253968 |Validation loss: 37773.263367132095 Valid flow loss: 125.7513425925926\n",
            "Epoch 7: Train loss: 104844.2084790002 Train flow loss: 126.33702976190476 |Validation loss: 38409.70511968085 Valid flow loss: 123.67652777777778\n",
            "Epoch 8: Train loss: 97884.57681251189 Train flow loss: 125.47249404761905 |Validation loss: 38136.050964788345 Valid flow loss: 122.61848148148148\n",
            "Epoch 9: Train loss: 100891.22842532641 Train flow loss: 124.69289087301587 |Validation loss: 37578.55234998338 Valid flow loss: 122.57894444444445\n",
            "Epoch 10: Train loss: 103400.35572886938 Train flow loss: 124.36474404761904 |Validation loss: 37611.99304285793 Valid flow loss: 123.21325\n",
            "Epoch 11: Train loss: 106670.94245192994 Train flow loss: 126.23799603174604 |Validation loss: 37987.84336560838 Valid flow loss: 123.61338888888889\n",
            "Epoch 12: Train loss: 100968.04236452983 Train flow loss: 126.58583134920634 |Validation loss: 38850.36405695922 Valid flow loss: 126.02327777777778\n",
            "Epoch 13: Train loss: 108169.27074935193 Train flow loss: 127.32799603174603 |Validation loss: 38744.53216596022 Valid flow loss: 127.09724074074074\n",
            "Epoch 14: Train loss: 101235.32113522047 Train flow loss: 128.25874404761905 |Validation loss: 38939.19509260029 Valid flow loss: 127.54558333333334\n",
            "Epoch 15: Train loss: 100818.33736964302 Train flow loss: 123.4120257936508 |Validation loss: 36130.83773998504 Valid flow loss: 121.96438888888889\n",
            "Epoch 16: Train loss: 101174.3384844404 Train flow loss: 124.97233928571428 |Validation loss: 37950.472477906136 Valid flow loss: 123.33867592592593\n",
            "Epoch 17: Train loss: 101594.81042201769 Train flow loss: 125.09083928571428 |Validation loss: 37147.31684431793 Valid flow loss: 122.02908333333333\n",
            "Epoch 18: Train loss: 99229.85456502093 Train flow loss: 125.67489682539683 |Validation loss: 36874.45209995568 Valid flow loss: 123.42825925925926\n",
            "Epoch 19: Train loss: 102147.80733179937 Train flow loss: 125.51368452380953 |Validation loss: 35950.37094657303 Valid flow loss: 122.55048148148148\n",
            "Epoch 20: Train loss: 96208.9149513651 Train flow loss: 125.54946031746032 |Validation loss: 38028.71843140514 Valid flow loss: 124.32325925925926\n",
            "Epoch 21: Train loss: 96865.17506666489 Train flow loss: 123.47763690476191 |Validation loss: 39022.49204898049 Valid flow loss: 122.06408333333333\n",
            "Epoch 22: Train loss: 94988.37785945527 Train flow loss: 124.18018253968253 |Validation loss: 37751.47919090758 Valid flow loss: 120.85155555555555\n",
            "Epoch 23: Train loss: 98962.04629976455 Train flow loss: 124.75867857142858 |Validation loss: 36057.54124418218 Valid flow loss: 123.7537037037037\n",
            "Epoch 24: Train loss: 100685.56648019822 Train flow loss: 123.22341468253968 |Validation loss: 37055.56373455508 Valid flow loss: 120.98055555555555\n",
            "Epoch 25: Train loss: 101833.07964595522 Train flow loss: 123.68075992063493 |Validation loss: 39161.588486258865 Valid flow loss: 121.5021111111111\n",
            "Epoch 26: Train loss: 102068.39132583595 Train flow loss: 123.9078630952381 |Validation loss: 36920.539547318265 Valid flow loss: 122.67643518518518\n",
            "Epoch 27: Train loss: 93877.12561351016 Train flow loss: 124.29348611111111 |Validation loss: 35061.54561100953 Valid flow loss: 121.35270370370371\n",
            "Epoch 28: Train loss: 102346.04165997788 Train flow loss: 123.70475595238095 |Validation loss: 36103.84703637522 Valid flow loss: 122.50273148148148\n",
            "Epoch 29: Train loss: 97974.07382262533 Train flow loss: 124.94274206349206 |Validation loss: 35458.22487256206 Valid flow loss: 120.85635185185185\n",
            "Epoch 30: Train loss: 102897.22226086853 Train flow loss: 123.86283134920635 |Validation loss: 36677.78249148105 Valid flow loss: 122.73037037037037\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zps38wjA96Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/Encoder_Decoder_2_Batchsize32_lr0\\.01_epoch28_betas\\(0.9,\\ 0.999\\) /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnkWMg6lyO7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_df = pd.read_csv('/content/dot_traffic_2015.txt.gz', compression='gzip', header=0, sep=',', quotechar='\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfUMvhHA1paj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64f39f3a-5f10-4b42-87f1-f93e9c651e00"
      },
      "source": [
        "test_df = new_df.iloc[25500:30000,:]\n",
        "test_df.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4500, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sghuhtyNBtmu",
        "colab_type": "code",
        "outputId": "6af5d899-4f3e-46e8-9edc-772c0a85e498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "val = np.array([])\n",
        "Encoder_2=AutoEncoder_2().to(device)\n",
        "Encoder_2.load_state_dict(torch.load('/content/drive/My Drive/Encoder_Decoder_2_Batchsize32_lr0.01_epoch28_betas(0.9, 0.999)'))\n",
        "test_loader =torch.utils.data.DataLoader(test_data,batch_size=32) \n",
        "for col in range(23):\n",
        "  col_data = []\n",
        "  for data in test_loader:\n",
        "    data_zero = zero_out_feature_fixed(data.clone(), col+1) # zero out one categorical feature\n",
        "    recon = Encoder_2(data_zero.to(device))\n",
        "    est = recon.cpu().detach().numpy().astype(np.int32)\n",
        "    col_data.append(est[:,col+1])\n",
        "  col_result = np.concatenate(col_data)\n",
        "  test_df['traffic_volume_estimate_after_'+str(col*100+100).zfill(4)+'_to_'+str(col*100+200).zfill(4)]=col_result"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ls08JWmB8AU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "6edcfeed-1613-40ec-ab84-6c01c9f751fa"
      },
      "source": [
        "test_df[:5]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day_of_data</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>direction_of_travel</th>\n",
              "      <th>direction_of_travel_name</th>\n",
              "      <th>fips_state_code</th>\n",
              "      <th>functional_classification</th>\n",
              "      <th>functional_classification_name</th>\n",
              "      <th>lane_of_travel</th>\n",
              "      <th>month_of_data</th>\n",
              "      <th>record_type</th>\n",
              "      <th>restrictions</th>\n",
              "      <th>station_id</th>\n",
              "      <th>traffic_volume_counted_after_0000_to_0100</th>\n",
              "      <th>traffic_volume_counted_after_0100_to_0200</th>\n",
              "      <th>traffic_volume_counted_after_0200_to_0300</th>\n",
              "      <th>traffic_volume_counted_after_0300_to_0400</th>\n",
              "      <th>traffic_volume_counted_after_0400_to_0500</th>\n",
              "      <th>traffic_volume_counted_after_0500_to_0600</th>\n",
              "      <th>traffic_volume_counted_after_0600_to_0700</th>\n",
              "      <th>traffic_volume_counted_after_0700_to_0800</th>\n",
              "      <th>traffic_volume_counted_after_0800_to_0900</th>\n",
              "      <th>traffic_volume_counted_after_0900_to_1000</th>\n",
              "      <th>traffic_volume_counted_after_1000_to_1100</th>\n",
              "      <th>traffic_volume_counted_after_1100_to_1200</th>\n",
              "      <th>traffic_volume_counted_after_1200_to_1300</th>\n",
              "      <th>traffic_volume_counted_after_1300_to_1400</th>\n",
              "      <th>traffic_volume_counted_after_1400_to_1500</th>\n",
              "      <th>traffic_volume_counted_after_1500_to_1600</th>\n",
              "      <th>traffic_volume_counted_after_1600_to_1700</th>\n",
              "      <th>traffic_volume_counted_after_1700_to_1800</th>\n",
              "      <th>traffic_volume_counted_after_1800_to_1900</th>\n",
              "      <th>traffic_volume_counted_after_1900_to_2000</th>\n",
              "      <th>traffic_volume_counted_after_2000_to_2100</th>\n",
              "      <th>traffic_volume_counted_after_2100_to_2200</th>\n",
              "      <th>traffic_volume_counted_after_2200_to_2300</th>\n",
              "      <th>traffic_volume_counted_after_2300_to_2400</th>\n",
              "      <th>year_of_data</th>\n",
              "      <th>traffic_volume_estimate_after_0100_to_0200</th>\n",
              "      <th>traffic_volume_estimate_after_0200_to_0300</th>\n",
              "      <th>traffic_volume_estimate_after_0300_to_0400</th>\n",
              "      <th>traffic_volume_estimate_after_0400_to_0500</th>\n",
              "      <th>traffic_volume_estimate_after_0500_to_0600</th>\n",
              "      <th>traffic_volume_estimate_after_0600_to_0700</th>\n",
              "      <th>traffic_volume_estimate_after_0700_to_0800</th>\n",
              "      <th>traffic_volume_estimate_after_0800_to_0900</th>\n",
              "      <th>traffic_volume_estimate_after_0900_to_1000</th>\n",
              "      <th>traffic_volume_estimate_after_1000_to_1100</th>\n",
              "      <th>traffic_volume_estimate_after_1100_to_1200</th>\n",
              "      <th>traffic_volume_estimate_after_1200_to_1300</th>\n",
              "      <th>traffic_volume_estimate_after_1300_to_1400</th>\n",
              "      <th>traffic_volume_estimate_after_1400_to_1500</th>\n",
              "      <th>traffic_volume_estimate_after_1500_to_1600</th>\n",
              "      <th>traffic_volume_estimate_after_1600_to_1700</th>\n",
              "      <th>traffic_volume_estimate_after_1700_to_1800</th>\n",
              "      <th>traffic_volume_estimate_after_1800_to_1900</th>\n",
              "      <th>traffic_volume_estimate_after_1900_to_2000</th>\n",
              "      <th>traffic_volume_estimate_after_2000_to_2100</th>\n",
              "      <th>traffic_volume_estimate_after_2100_to_2200</th>\n",
              "      <th>traffic_volume_estimate_after_2200_to_2300</th>\n",
              "      <th>traffic_volume_estimate_after_2300_to_2400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25500</th>\n",
              "      <td>2015-04-21</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>East</td>\n",
              "      <td>51</td>\n",
              "      <td>1R</td>\n",
              "      <td>Rural: Principal Arterial - Interstate</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>782989</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>58</td>\n",
              "      <td>137</td>\n",
              "      <td>144</td>\n",
              "      <td>135</td>\n",
              "      <td>97</td>\n",
              "      <td>100</td>\n",
              "      <td>84</td>\n",
              "      <td>85</td>\n",
              "      <td>64</td>\n",
              "      <td>61</td>\n",
              "      <td>52</td>\n",
              "      <td>71</td>\n",
              "      <td>76</td>\n",
              "      <td>87</td>\n",
              "      <td>48</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>27</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>56</td>\n",
              "      <td>127</td>\n",
              "      <td>139</td>\n",
              "      <td>113</td>\n",
              "      <td>151</td>\n",
              "      <td>131</td>\n",
              "      <td>121</td>\n",
              "      <td>109</td>\n",
              "      <td>94</td>\n",
              "      <td>89</td>\n",
              "      <td>83</td>\n",
              "      <td>69</td>\n",
              "      <td>57</td>\n",
              "      <td>47</td>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "      <td>34</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25501</th>\n",
              "      <td>2015-07-08</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>North</td>\n",
              "      <td>13</td>\n",
              "      <td>1U</td>\n",
              "      <td>Urban: Principal Arterial - Interstate</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>000337</td>\n",
              "      <td>352</td>\n",
              "      <td>222</td>\n",
              "      <td>188</td>\n",
              "      <td>206</td>\n",
              "      <td>288</td>\n",
              "      <td>510</td>\n",
              "      <td>810</td>\n",
              "      <td>942</td>\n",
              "      <td>939</td>\n",
              "      <td>1151</td>\n",
              "      <td>1244</td>\n",
              "      <td>1247</td>\n",
              "      <td>1326</td>\n",
              "      <td>1376</td>\n",
              "      <td>1453</td>\n",
              "      <td>1678</td>\n",
              "      <td>1843</td>\n",
              "      <td>1777</td>\n",
              "      <td>1493</td>\n",
              "      <td>1238</td>\n",
              "      <td>958</td>\n",
              "      <td>788</td>\n",
              "      <td>635</td>\n",
              "      <td>480</td>\n",
              "      <td>15</td>\n",
              "      <td>409</td>\n",
              "      <td>239</td>\n",
              "      <td>189</td>\n",
              "      <td>191</td>\n",
              "      <td>464</td>\n",
              "      <td>844</td>\n",
              "      <td>1306</td>\n",
              "      <td>973</td>\n",
              "      <td>1081</td>\n",
              "      <td>1270</td>\n",
              "      <td>1419</td>\n",
              "      <td>1423</td>\n",
              "      <td>1459</td>\n",
              "      <td>1499</td>\n",
              "      <td>1500</td>\n",
              "      <td>1469</td>\n",
              "      <td>1443</td>\n",
              "      <td>1287</td>\n",
              "      <td>1091</td>\n",
              "      <td>947</td>\n",
              "      <td>815</td>\n",
              "      <td>662</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25502</th>\n",
              "      <td>2015-01-22</td>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>South</td>\n",
              "      <td>1</td>\n",
              "      <td>1U</td>\n",
              "      <td>Urban: Principal Arterial - Interstate</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>000417</td>\n",
              "      <td>85</td>\n",
              "      <td>60</td>\n",
              "      <td>91</td>\n",
              "      <td>119</td>\n",
              "      <td>204</td>\n",
              "      <td>465</td>\n",
              "      <td>790</td>\n",
              "      <td>1078</td>\n",
              "      <td>868</td>\n",
              "      <td>656</td>\n",
              "      <td>628</td>\n",
              "      <td>708</td>\n",
              "      <td>729</td>\n",
              "      <td>783</td>\n",
              "      <td>954</td>\n",
              "      <td>1230</td>\n",
              "      <td>1392</td>\n",
              "      <td>1277</td>\n",
              "      <td>774</td>\n",
              "      <td>575</td>\n",
              "      <td>423</td>\n",
              "      <td>330</td>\n",
              "      <td>207</td>\n",
              "      <td>159</td>\n",
              "      <td>15</td>\n",
              "      <td>35</td>\n",
              "      <td>73</td>\n",
              "      <td>26</td>\n",
              "      <td>367</td>\n",
              "      <td>443</td>\n",
              "      <td>677</td>\n",
              "      <td>872</td>\n",
              "      <td>796</td>\n",
              "      <td>770</td>\n",
              "      <td>736</td>\n",
              "      <td>718</td>\n",
              "      <td>725</td>\n",
              "      <td>762</td>\n",
              "      <td>819</td>\n",
              "      <td>957</td>\n",
              "      <td>1101</td>\n",
              "      <td>1108</td>\n",
              "      <td>870</td>\n",
              "      <td>648</td>\n",
              "      <td>529</td>\n",
              "      <td>441</td>\n",
              "      <td>344</td>\n",
              "      <td>252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25503</th>\n",
              "      <td>2015-08-05</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>East</td>\n",
              "      <td>12</td>\n",
              "      <td>3U</td>\n",
              "      <td>Urban: Principal Arterial - Other</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>860214</td>\n",
              "      <td>85</td>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>42</td>\n",
              "      <td>61</td>\n",
              "      <td>220</td>\n",
              "      <td>599</td>\n",
              "      <td>830</td>\n",
              "      <td>864</td>\n",
              "      <td>614</td>\n",
              "      <td>532</td>\n",
              "      <td>546</td>\n",
              "      <td>577</td>\n",
              "      <td>600</td>\n",
              "      <td>620</td>\n",
              "      <td>600</td>\n",
              "      <td>617</td>\n",
              "      <td>661</td>\n",
              "      <td>583</td>\n",
              "      <td>431</td>\n",
              "      <td>375</td>\n",
              "      <td>319</td>\n",
              "      <td>268</td>\n",
              "      <td>136</td>\n",
              "      <td>15</td>\n",
              "      <td>89</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>184</td>\n",
              "      <td>368</td>\n",
              "      <td>626</td>\n",
              "      <td>653</td>\n",
              "      <td>690</td>\n",
              "      <td>680</td>\n",
              "      <td>661</td>\n",
              "      <td>650</td>\n",
              "      <td>657</td>\n",
              "      <td>711</td>\n",
              "      <td>744</td>\n",
              "      <td>767</td>\n",
              "      <td>724</td>\n",
              "      <td>554</td>\n",
              "      <td>416</td>\n",
              "      <td>334</td>\n",
              "      <td>272</td>\n",
              "      <td>201</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25504</th>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>17</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>North</td>\n",
              "      <td>55</td>\n",
              "      <td>4R</td>\n",
              "      <td>Rural: Minor Arterial</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>010001</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>34</td>\n",
              "      <td>62</td>\n",
              "      <td>110</td>\n",
              "      <td>110</td>\n",
              "      <td>122</td>\n",
              "      <td>124</td>\n",
              "      <td>141</td>\n",
              "      <td>115</td>\n",
              "      <td>125</td>\n",
              "      <td>101</td>\n",
              "      <td>125</td>\n",
              "      <td>190</td>\n",
              "      <td>156</td>\n",
              "      <td>74</td>\n",
              "      <td>64</td>\n",
              "      <td>42</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>7</td>\n",
              "      <td>45</td>\n",
              "      <td>118</td>\n",
              "      <td>148</td>\n",
              "      <td>161</td>\n",
              "      <td>191</td>\n",
              "      <td>159</td>\n",
              "      <td>152</td>\n",
              "      <td>138</td>\n",
              "      <td>122</td>\n",
              "      <td>112</td>\n",
              "      <td>94</td>\n",
              "      <td>76</td>\n",
              "      <td>66</td>\n",
              "      <td>64</td>\n",
              "      <td>49</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             date  ...  traffic_volume_estimate_after_2300_to_2400\n",
              "25500  2015-04-21  ...                                           0\n",
              "25501  2015-07-08  ...                                         480\n",
              "25502  2015-01-22  ...                                         252\n",
              "25503  2015-08-05  ...                                         136\n",
              "25504  2015-09-17  ...                                          25\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LMJ8Ebd4OPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.to_csv('estimated.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYWkad5T426v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}